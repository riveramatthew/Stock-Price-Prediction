


# TODO: Install any packages that you might need


# TODO: Import any packages that you might need
import json
import os
import boto3
import random
from botocore import UNSIGNED
from botocore.config import Config


s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))


import boto3
import json

s3 = boto3.client('s3')
bucket_name = 'sagemaker-inventory-project-20251009'  # or similar

# List objects to see the structure
response = s3.list_objects_v2(Bucket=bucket_name, MaxKeys=20)

if 'Contents' in response:
    print("Found objects:")
    for obj in response['Contents']:
        print(f"  Key: {obj['Key']}")
else:
    print("No objects found or bucket doesn't exist")
    
# Try to list buckets you have access to
print("\nAvailable buckets:")
buckets = s3.list_buckets()
for bucket in buckets['Buckets']:
    if 'sagemaker' in bucket['Name'].lower() or 'inventory' in bucket['Name'].lower():
        print(f"  - {bucket['Name']}")


import boto3
import json
from collections import defaultdict

def explore_bucket_structure(bucket_name):
    """Explore the full bucket structure"""
    s3 = boto3.client('s3')
    
    # Get all objects
    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket=bucket_name)
    
    all_keys = []
    for page in pages:
        if 'Contents' in page:
            for obj in page['Contents']:
                all_keys.append(obj['Key'])
    
    # Organize by directory
    structure = defaultdict(list)
    for key in all_keys:
        parts = key.split('/')
        if len(parts) > 1:
            directory = '/'.join(parts[:-1])
            structure[directory].append(parts[-1])
    
    print("Bucket structure:")
    for directory in sorted(structure.keys()):
        file_count = len(structure[directory])
        print(f"  {directory}/ ({file_count} files)")
        # Show first few files
        for f in structure[directory][:3]:
            print(f"    - {f}")
        if file_count > 3:
            print(f"    ... and {file_count - 3} more")
    
    return all_keys, structure

bucket_name = 'sagemaker-inventory-project-20251009'
all_keys, structure = explore_bucket_structure(bucket_name)

# Check for any CSV or JSON files that might contain labels
print("\n\nMetadata files (CSV/JSON):")
for key in all_keys:
    if key.endswith(('.csv', '.json', '.txt')):
        print(f"  {key}")





import os
import boto3
import pandas as pd
from tqdm import tqdm

def download_dataset_from_s3(bucket_name):
    """Download the complete dataset with existing splits from S3"""
    s3 = boto3.client('s3')
    
    # Create local directory structure
    os.makedirs('data/train/images', exist_ok=True)
    os.makedirs('data/val/images', exist_ok=True)
    os.makedirs('data/test/images', exist_ok=True)
    
    splits = ['train', 'val', 'test']
    
    for split in splits:
        print(f"\n{'='*50}")
        print(f"Downloading {split} split")
        print('='*50)
        
        # Download CSV file
        csv_key = f'data/{split}/{split}.csv'
        csv_local = f'data/{split}.csv'
        
        print(f"\nDownloading {csv_key}...")
        try:
            s3.download_file(bucket_name, csv_key, csv_local)
            
            # Read CSV to see structure
            df = pd.read_csv(csv_local)
            print(f"✓ Downloaded {split}.csv")
            print(f"  Shape: {df.shape}")
            print(f"  Columns: {list(df.columns)}")
            print(f"  First few rows:")
            print(df.head())
            
            # Check if num_objects column exists
            if 'num_objects' in df.columns:
                print(f"  Label distribution: {df['num_objects'].value_counts().sort_index().to_dict()}")
        except Exception as e:
            print(f"Error downloading CSV: {e}")
            continue
        
        # Download images
        print(f"\nDownloading {split} images...")
        image_prefix = f'data/{split}/images/'
        
        # List all images in this split
        paginator = s3.get_paginator('list_objects_v2')
        pages = paginator.paginate(Bucket=bucket_name, Prefix=image_prefix)
        
        image_keys = []
        for page in pages:
            if 'Contents' in page:
                for obj in page['Contents']:
                    if obj['Key'].endswith('.jpg'):
                        image_keys.append(obj['Key'])
        
        print(f"Found {len(image_keys)} images to download")
        
        # Download each image
        success_count = 0
        for key in tqdm(image_keys, desc=f"{split} images"):
            filename = os.path.basename(key)
            local_path = f'data/{split}/images/{filename}'
            try:
                s3.download_file(bucket_name, key, local_path)
                success_count += 1
            except Exception as e:
                print(f"\nError downloading {key}: {e}")
        
        print(f"✓ Successfully downloaded {success_count}/{len(image_keys)} {split} images")
    
    print("\n" + "="*50)
    print("Download complete!")
    print("="*50)
    
    # Final summary
    print("\nDataset summary:")
    for split in splits:
        csv_path = f'data/{split}.csv'
        img_dir = f'data/{split}/images/'
        
        if os.path.exists(csv_path):
            df = pd.read_csv(csv_path)
            img_count = len([f for f in os.listdir(img_dir) if f.endswith('.jpg')]) if os.path.exists(img_dir) else 0
            print(f"\n  {split.upper()}:")
            print(f"    CSV entries: {len(df)}")
            print(f"    Images downloaded: {img_count}")
            if 'num_objects' in df.columns:
                print(f"    Label distribution: {df['num_objects'].value_counts().sort_index().to_dict()}")

if __name__ == "__main__":
    bucket_name = 'sagemaker-inventory-project-20251009'
    print(f"Starting download from bucket: {bucket_name}\n")
    download_dataset_from_s3(bucket_name)
    
    print("\n✅ All done! Your data is ready in the 'data/' directory")
    print("\nYou should now have:")
    print("  - data/train.csv & data/train/images/ (4000 images)")
    print("  - data/val.csv & data/val/images/ (500 images)")
    print("  - data/test.csv & data/test/images/ (500 images)")


# Sample file_list.json generator (small subset; expand as needed)
sample_file_list = {
    "1": ["00000001.jpg", "00000002.jpg", "00000003.jpg", "00000004.jpg", "00000005.jpg"],  # Replace with real IDs
    "2": ["00000006.jpg", "00000007.jpg", "00000008.jpg", "00000009.jpg", "00000010.jpg"],
    # Add up to "10": [...] with ~1000 each for full subset
    # To get real IDs: Use AWS CLI locally: aws s3 ls s3://aft-vbi-pds/bin-images/ --no-sign-request | awk '{print $4}' > all_files.txt
    # Then group by metadata (requires full metadata download separately)
}

with open('file_list.json', 'w') as f:
    json.dump(sample_file_list, f, indent=2)
print("Sample file_list.json created. For full, download from Kaggle: https://www.kaggle.com/datasets/williamhyun/amazon-bin-image-dataset-file-list")


import pandas as pd
train_df = pd.read_csv('data/train/train.csv')
print(train_df.head())
print(train_df['num_objects'].value_counts().sort_index())  # Balanced?





#TODO: Perform any data cleaning or data preprocessing


import os
import pandas as pd
from PIL import Image

def verify_dataset():
    """Verify the downloaded dataset"""
    splits = ['train', 'val', 'test']
    
    print("Dataset Verification\n" + "="*50)
    
    for split in splits:
        csv_path = f'data/{split}.csv'
        img_dir = f'data/{split}/images/'
        
        # Load CSV
        df = pd.read_csv(csv_path)
        
        # Check for missing images
        missing = []
        for idx, row in df.iterrows():
            img_path = os.path.join(img_dir, f"{row['image_id']:05d}.jpg")
            if not os.path.exists(img_path):
                missing.append(row['image_id'])
        
        print(f"\n{split.upper()}:")
        print(f"  ✓ CSV entries: {len(df)}")
        print(f"  ✓ Images found: {len(os.listdir(img_dir))}")
        print(f"  ✓ Missing images: {len(missing)}")
        
        if len(missing) > 0:
            print(f"    Missing IDs: {missing[:10]}...")
        
        # Try loading a sample image
        sample_id = df.iloc[0]['image_id']
        sample_path = os.path.join(img_dir, f"{sample_id:05d}.jpg")
        try:
            img = Image.open(sample_path)
            print(f"  ✓ Sample image loaded: {img.size}, {img.mode}")
        except Exception as e:
            print(f"  ✗ Error loading sample: {e}")
    
    print("\n" + "="*50)
    print("✅ Verification complete!")

if __name__ == "__main__":
    verify_dataset()


import pandas as pd
import os

# Check train data
train_df = pd.read_csv('data/train.csv')
train_images = sorted(os.listdir('data/train/images/'))

print("CSV image_ids (first 10):")
print(train_df['image_id'].head(10).tolist())

print("\nActual filenames (first 10):")
print(train_images[:10])

# Check if we need to pad with zeros
sample_id = train_df.iloc[0]['image_id']
sample_filename = train_images[0]

print(f"\nFirst CSV id: {sample_id}")
print(f"First file: {sample_filename}")
print(f"Expected filename for id {sample_id}: {sample_id:05d}.jpg or {sample_id:06d}.jpg")


import pandas as pd
import os
import re

def investigate_mismatch():
    """Figure out the relationship between CSV IDs and filenames"""
    
    # Load data
    train_df = pd.read_csv('data/train.csv')
    train_images = sorted(os.listdir('data/train/images/'))
    
    print("Investigation Results:")
    print("="*60)
    print(f"CSV has {len(train_df)} entries")
    print(f"Directory has {len(train_images)} images")
    print()
    
    # Check if filenames might be indices
    print("Hypothesis 1: Filenames are just sequential indices")
    print("-"*60)
    
    # Extract numbers from filenames
    file_numbers = []
    for fname in train_images[:20]:
        num = int(re.findall(r'\d+', fname)[0])
        file_numbers.append(num)
    
    print(f"First 20 file numbers: {file_numbers}")
    print(f"Are they sequential? {file_numbers == list(range(file_numbers[0], file_numbers[0] + 20))}")
    print()
    
    # Check if CSV might be sorted differently
    print("Hypothesis 2: CSV and files are in different orders")
    print("-"*60)
    print(f"CSV IDs sorted: {sorted(train_df['image_id'].head(10).tolist())}")
    print()
    
    # Check if we should look for a mapping file
    print("Hypothesis 3: There might be a mapping file in S3")
    print("-"*60)
    print("We should check S3 for any index/mapping files...")
    print()
    
    # Most likely scenario
    print("Most Likely Scenario:")
    print("-"*60)
    print("The CSV 'image_id' refers to the original dataset ID,")
    print("but the downloaded files were renamed sequentially.")
    print("We need to either:")
    print("  1. Find the original mapping")
    print("  2. Re-download with correct names")
    print("  3. Create a new mapping based on download order")

investigate_mismatch()

# Let's also check the S3 keys to see the original filenames
print("\n" + "="*60)
print("Checking original S3 filenames...")
print("="*60)

import boto3

s3 = boto3.client('s3')
bucket_name = 'sagemaker-inventory-project-20251009'

# Get a few original S3 keys
response = s3.list_objects_v2(
    Bucket=bucket_name,
    Prefix='data/train/images/',
    MaxKeys=20
)

if 'Contents' in response:
    print("\nFirst 20 S3 filenames:")
    for obj in response['Contents']:
        if obj['Key'].endswith('.jpg'):
            filename = os.path.basename(obj['Key'])
            print(f"  {filename}")


import boto3

s3 = boto3.client('s3')
bucket_name = 'sagemaker-inventory-project-20251009'

# Look for any mapping or index files
print("Searching for mapping/index files in S3...")
print("="*60)

response = s3.list_objects_v2(Bucket=bucket_name, Prefix='data/')

relevant_files = []
if 'Contents' in response:
    for obj in response['Contents']:
        key = obj['Key']
        # Look for any file that might contain mappings
        if any(x in key.lower() for x in ['map', 'index', 'manifest', 'metadata', 'info']):
            relevant_files.append(key)
        # Also check for any JSON/TXT files at the data level
        if key.count('/') <= 2 and (key.endswith('.json') or key.endswith('.txt') or key.endswith('.csv')):
            if key not in ['data/train/train.csv', 'data/val/val.csv', 'data/test/test.csv']:
                relevant_files.append(key)

if relevant_files:
    print("Found potential mapping files:")
    for f in relevant_files:
        print(f"  {f}")
else:
    print("No mapping files found.")
    print("\nThis means we need to create our own mapping or fix the CSVs.")


import os
import pandas as pd
import re

def extract_number_from_filename(filename):
    """Extract the numeric part from filename like '00004.jpg'"""
    match = re.findall(r'\d+', filename)
    return int(match[0]) if match else None

def fix_csv_files():
    """Recreate CSV files to match actual filenames"""
    
    splits = ['train', 'val', 'test']
    
    for split in splits:
        print(f"\n{'='*60}")
        print(f"Fixing {split} CSV")
        print('='*60)
        
        # Load original CSV to get labels
        original_csv = pd.read_csv(f'data/{split}.csv')
        print(f"Original CSV has {len(original_csv)} entries")
        
        # Get actual image files
        img_dir = f'data/{split}/images/'
        image_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.jpg')])
        print(f"Found {len(image_files)} actual images")
        
        # Create new mapping
        new_data = []
        for i, filename in enumerate(image_files):
            # Get the corresponding label from original CSV (by row order)
            if i < len(original_csv):
                num_objects = original_csv.iloc[i]['num_objects']
            else:
                print(f"Warning: No label for {filename}")
                continue
            
            # Remove .jpg extension for image_id
            image_id = filename.replace('.jpg', '')
            
            new_data.append({
                'image_id': image_id,
                'num_objects': num_objects
            })
        
        # Create new DataFrame
        new_df = pd.DataFrame(new_data)
        
        # Save fixed CSV
        new_df.to_csv(f'data/{split}.csv', index=False)
        
        print(f"✓ Created new CSV with {len(new_df)} entries")
        print(f"  Columns: {list(new_df.columns)}")
        print(f"  Label distribution: {new_df['num_objects'].value_counts().sort_index().to_dict()}")
        print(f"\nFirst few rows:")
        print(new_df.head(10))
    
    print("\n" + "="*60)
    print("✅ All CSV files fixed!")
    print("="*60)

def verify_fixed_dataset():
    """Verify that CSVs now match the actual images"""
    
    splits = ['train', 'val', 'test']
    all_good = True
    
    print("\n" + "="*60)
    print("VERIFICATION")
    print("="*60)
    
    for split in splits:
        print(f"\n{split.upper()}:")
        
        df = pd.read_csv(f'data/{split}.csv')
        img_dir = f'data/{split}/images/'
        
        # Check each entry
        missing = []
        for idx, row in df.iterrows():
            img_path = os.path.join(img_dir, f"{row['image_id']}.jpg")
            if not os.path.exists(img_path):
                missing.append(row['image_id'])
        
        if len(missing) == 0:
            print(f"  ✓ All {len(df)} images found!")
            print(f"  ✓ Label distribution: {df['num_objects'].value_counts().sort_index().to_dict()}")
        else:
            print(f"  ✗ Missing {len(missing)} images")
            print(f"    First few missing: {missing[:5]}")
            all_good = False
    
    if all_good:
        print("\n" + "="*60)
        print("✅ Dataset is ready for training!")
        print("="*60)
    else:
        print("\n" + "="*60)
        print("⚠️  Some issues found - review above")
        print("="*60)

if __name__ == "__main__":
    print("Fixing CSV files to match downloaded images...")
    fix_csv_files()
    verify_fixed_dataset()





#TODO: Upload the data to AWS S3
get_ipython().getoutput("aws s3 mb s3://sagemaker-inventory-project-20251009 --region us-east-1")


get_ipython().getoutput("aws s3 sync data/ s3://sagemaker-inventory-project-20251009/data/ --region us-east-1")


get_ipython().getoutput("aws s3 ls s3://sagemaker-inventory-project-20251009/data/train/ --recursive | head -5")





get_ipython().getoutput("pip install torch torchvision sagemaker pandas pillow scikit-learn")


#TODO: Declare your model training hyperparameter.
#NOTE: You do not need to do hyperparameter tuning. You can use fixed hyperparameter values
hyperparameters = {
    'epochs': 10,
    'batch-size': 32,
    'learning-rate': 0.001,
    'num-classes': 10  # 1-10 objects
}


import argparse
import json
import logging
import os
import sys

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms, models
import pandas as pd
from PIL import Image

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BinDataset(torch.utils.data.Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        try:
            self.annotations = pd.read_csv(csv_file)
            logger.info(f"Loaded {len(self.annotations)} samples from {csv_file}")
            if len(self.annotations) == 0:
                raise ValueError("Empty CSV - no data loaded!")
        except Exception as e:
            logger.error(f"Error loading CSV {csv_file}: {e}")
            raise
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        img_name = os.path.join(self.root_dir, f"images/{self.annotations.iloc[idx, 0]}.jpg")
        try:
            image = Image.open(img_name).convert("RGB")
            label = self.annotations.iloc[idx, 1] - 1  # Map 1-10 to 0-9 for 10 classes
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            logger.error(f"Error loading {img_name}: {e}")
            raise

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--batch-size", type=int, default=32)
    parser.add_argument("--learning-rate", type=float, default=0.001)
    parser.add_argument("--num-classes", type=int, default=10)
    parser.add_argument("--model-dir", type=str, default=os.environ.get("SM_MODEL_DIR", "/tmp"))
    parser.add_argument("--train", type=str, default=os.environ.get("SM_CHANNEL_TRAIN"))
    parser.add_argument("--validation", type=str, default=os.environ.get("SM_CHANNEL_VALIDATION"))
    
    # Fixed: Use parse_known_args() to ignore Jupyter extras (e.g., -f kernel.json)
    args, unknown = parser.parse_known_args()
    logger.info(f"Args: {args}")
    logger.info(f"Ignored extras: {unknown}")  # Debug: Shows Jupyter args

    # Transforms
    train_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # Datasets/Loaders
    train_csv = os.path.join(args.train, 'train.csv')
    val_csv = os.path.join(args.validation, 'val.csv')
    train_dataset = BinDataset(train_csv, args.train, train_transform)
    val_dataset = BinDataset(val_csv, args.validation, val_transform)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, num_workers=0)

    # Model
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    logger.info(f"Using device: {device}")
    model = models.resnet18(pretrained=True)
    model.fc = nn.Linear(model.fc.in_features, args.num_classes)
    model = model.to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    # Training loop
    for epoch in range(args.epochs):
        model.train()
        running_loss = 0.0
        for batch_idx, (inputs, labels) in enumerate(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            if batch_idx % 10 == 0:
                logger.info(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")
        scheduler.step()

        # Validation
        model.eval()
        val_loss = 0.0
        correct = 0
        total = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                val_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        avg_train_loss = running_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)
        accuracy = 100 * correct / total
        logger.info(f"Epoch {epoch+1}: Train Loss {avg_train_loss:.4f}, Val Loss {avg_val_loss:.4f}, Accuracy {accuracy:.2f}%")

        # SageMaker metrics
        if os.path.exists("/opt/ml/output/tuning"):
            with open(os.path.join("/opt/ml/output/tuning", "metric_definitions.json"), "w") as f:
                json.dump([{"Name": "accuracy", "Regex": "Accuracy ([0-9\\.]+)%"}], f)

    # Save
    os.makedirs(args.model_dir, exist_ok=True)
    torch.save(model.state_dict(), os.path.join(args.model_dir, "model.pth"))
    logger.info("Training complete.")

if __name__ == "__main__":
    main()


estimator.fit(inputs, wait=True)








#TODO: Create your hyperparameter search space


#TODO: Create your training estimator


# TODO: Fit your estimator


# TODO: Find the best hyperparameters





# TODO: Set up debugging and profiling rules and hooks


# TODO: Create and fit an estimator


# TODO: Plot a debugging output.





# TODO: Display the profiler output





# TODO: Deploy your model to an endpoint


# TODO: Run an prediction on the endpoint


# TODO: Remember to shutdown/delete your endpoint once your work is done





# TODO: Cost Analysis


# TODO: Train your model using a spot instance





# TODO: Train your model on Multiple Instances
